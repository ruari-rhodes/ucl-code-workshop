---
title: "US Urban growth"
author: "Ruari Rhodes"
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: hide
---


# Multi-lingual spatial data workflows

## R, Python and Earth Engine

This document will walk you through some uses for an interactive R/Python script, with calls to the Earth Engine API embedded. It assumes working knowledge of all three languages, but should be straightforward to follow the majority of the process.

As an example, we will investigate the 20 largest urban areas by population in the USA. Urban area statistics are captured by the US census in "Census Statistical Areas" (CSA), which encompass conurbations.

As always, we will start by loading libraries:



```{r load_libraries, message=FALSE, warning=FALSE}
library( tidyverse)
library( sf)
library( here)
library( googledrive)
library( gridExtra)
library( reticulate)
use_python("/anaconda3/bin/python")
```


## Simple Features and spatial data in R

We start by loading in some Census Statistical Area (CSA) population data (originally pulled from Wikipedia). Our populations are stored in a CSV file, and we'll be joining them onto a Simple Features dataframe of spatial polygons.



```{r get_populous_areas, message=FALSE}

( urban_areas <- read_csv( here( 'data_in', 'populous_urban_areas.csv')) %>% 
    select( Rank, CSA, Pop = `2017 Estimate`) %>% 
    mutate( CSA = gsub( " CSA", "", CSA)) )

```

Now we have our population data for each CSA, we can join them to a spatial polygon database (a "Simple Feature") (freely available from the US Gov't TIGER shapefile database:

```{r get_csas}
csa <- read_sf( here( 'data_in', 'cb_2017_us_csa_500k', 'cb_2017_us_csa_500k.shp'))
```

csa is a "SimpleFeatures" object. This is a normal dataframe with a geometry column attached; in this case the geometry is a polygon demarcating the boundaries of each region.

Let's investigate the data inside csa:

```{r view_csas}
head( csa)
```


We can see that NAME contains the standard format CSA name as we have in the population dataset, meaning we can join the two using inner_join(). We'll also filter the data to the 20 most populous regions.

```{r join_csa_data}

# How many CSAs to keep?
n_csa <- 20


(csa_filtered <- csa %>% 
    inner_join( urban_areas, by=c("NAME" = "CSA")) %>% 
    arrange( desc(Pop)) %>% 
    head(    n_csa) %>% 
    select(  NAME, Pop) )
```


## Visualising in R

So, where are these regions?

*R top tip:* ggplot + geom_sf() will plot **sf** objects within the grammar of graphics framework (i.e. in a simple, readable, layerable way)

```{r visualise_csas, fig.width=7, fig.height=5}

# Load a state shapefile, and filter out states (and territories) that aren't in the lower 48
states_sf <- read_sf( here( 'data_in/cb_2017_us_state_20m/cb_2017_us_state_20m.shp')) %>% 
  filter( !(STUSPS %in% c( "AK", "HI", "PR")))

# Plot a map of the USA. Overlay CSA regions coloured by population.
ggplot(csa_filtered) +
  geom_sf(data = states_sf, fill = "white") +
  geom_sf(aes(fill = Pop / 1e6)) +
  coord_sf(crs = 5070) +
  scale_fill_distiller(
    "Population (m)",
    palette = "OrRd",
    labels = scales::comma,
    direction = 1,
    trans  = "log10",
    limits = c(0.5, 30),
    breaks = c( 1, 2, 5, 10, 20, 30)
  ) +
  theme_bw() + 
  ggtitle( sprintf( "Top %i CSA regions by total population", n_csa))
```


## Transfer to Python

So we've got the top 20 census urban regions by population. But R isn't going to be powerful enough to analyse land use changes for these regions, so it's time to bring in Earth Engine.

We can use R to create a plaintext version of an Earth Engine geometry, which can be passed to Python to use its Earth Engine API.

We can either write this plaintext to a file to re-import to the Python interpreter, or we can pass it directly to python using the "r.data" structure


```{r geometry_to_plaintext, results='hide'}
csa_filtered %>% 
  split( .$NAME) %>% 
  walk( function(x){
    if( nrow(x) > 1){ ## If a CSA is multipolygon, take the poly with the highest population
                      ## (This is a bit of a hack and should be fixed with st_union)
      x <- x %>% slice( which( .$Pop == max( .$Pop)))
    }
    coord_mat <- x %>% st_coordinates() %>% .[,c("X","Y")] %>% t()
    coord_out <- coord_mat %>% as.vector() %>% paste( collapse=" ")
    #coord_out <- paste( "[", coord_out, "]")
    filename <- paste0( x$NAME, ".txt")
    write_lines( coord_out, here('data_out', 'csa_wkt', filename))
  }) 


```


Here we have output everything to text files for future reference, but note that we could simply have transferred data from an R chunk to a Python chunk. For example, if we create a dataframe in R called test data:

```{r data_to_python}
test_data <- data.frame( a=1:10, b=11:20)
```


We can access it in Python using r.test_data:

```{python data_in_python}
test_data = r.test_data

test_data.describe()
```

And we can send it back to R again as py$test_data
```{r data_to_r}
py$test_data %>% glimpse()
```



## Accessing Earth Engine through Python

Earth Engine has a Python API which can directly access the full functionality of Earth Engine (except for visualisations). So for any big number-crunching projects that need to be repeatable / scriptable, this is a great way to go. We need to install the Earth Engine API through:

```
pip install earthengine-api
```

and import the package to Python as ee, followed by ee.Initialize(). The first time you do this, a browser will pop up asking you to authorise access to your account.

```{python ee_setup}

# Import the Earth Engine Python Package
import ee
import numpy as np
import pandas as pd
import os

# Initialize the Earth Engine object, using the authentication credentials.
ee.Initialize()

```


Let's try taking a sample of 10,000 land use/land cover points within our CSA regions.


```{python ee_function}

# Load in the plaintext geometries that we saved earlier
coord_directory = os.getcwd() + "/data_out/csa_wkt"
files           = os.listdir( coord_directory)
files           = np.array( files)[ [f.endswith(".txt") for f in files] ]
output_names    = [f.split(".")[0].split(",")[0] for f in files]
paths           = [coord_directory + "/" + f for f in files]


# Iterate over all of our geometries
for i in range(0,len(files)):
  
  # Open the text file and convert lon/lat string to Earth Engine geometry object
  f = open( paths[i], 'r')
  urban_geometry_raw = f.read()
  urban_geometry     = [ float(n) for n in urban_geometry_raw.split()]
  urban_geometry_ee  = ee.Geometry.Polygon(urban_geometry)
  
  # Sample 10k points from each region
  sample = ee.Image( 'USGS/NLCD/NLCD2011').sample( region=urban_geometry_ee,
                                                   numPixels=1e5,
                                                   seed=1234)
  
  # Export to Drive
  export_job = ee.batch.Export.table.toDrive(sample,
                                folder="sample_export",
                                description=output_names[i],
                                fileFormat="CSV"
                                )
  export_job.start()
  
  # Uncomment if you want to view your previously submitted jobs, and any errors.
  #print( ee.batch.Task.list() )
```


Using the "googledrive" package, we can list the contents of our "sample_export" directory to make sure that the code above has worked.

```{r googledrive_setup}
drive_ls( "sample_export")
```


Looks like the data are in the right place. Using the GoogleDrive package, we'll load in the first result.


```{r get_data_from_drive, warning=FALSE, message=FALSE}
target_drive_file <- drive_ls( "sample_export") %>% slice(1)
target_local_file <- here( 
  file.path(
    "data_out", target_drive_file$name
  ))

drive_download( file = target_drive_file, path = target_local_file, overwrite=TRUE )

csa_nlcd_data <- read_csv( target_local_file,
   col_types =  cols(
     `system:index` = col_integer(),
     impervious = col_integer(),
     landcover = col_integer(),
     percent_tree_cover = col_integer(),
     percent_tree_error = col_integer(),
     .geo = col_character()
  )
)
```

Voila - we have data from Google Drive. Let's take a look:
```{r view_drive_data, fig.width=6, fig.height=4, message=FALSE, warning=FALSE}
glimpse( csa_nlcd_data)


plots <- csa_nlcd_data %>% 
  select( -c(`system:index`, .geo)) %>% 
  map2( names(.), function(x,y){  qplot(unlist(x)) + theme_classic() + ggtitle(y)} )


grid.arrange( grobs=plots, nrow=2)


```

In a few easy steps, we have imported spatial data into R, used it to drive an Earth Engine function via a Python API, and dragged that data back to R for analysis. Of course, this data is now available in the Python interpreter as well:

```{python view_drive_data_python}
print( r.csa_nlcd_data.head())
```

So if you're happier doing your statistical modelling in R, and plotting with matplotlib, you can easily do this. Likewise, if scikitlearn is your tool of choice but you prefer the grammar of graphics with ggplot, feel free. Use the best tool for the job, in the way in which you are most comfortable: be fickle.















